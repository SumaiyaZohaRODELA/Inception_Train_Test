{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJl8TcB7UsH8kjNoWywIEl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumaiyaZohaRODELA/Inception_Train_Test/blob/main/inception_train_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "iRIkvDlQASXz",
        "outputId": "7bdeeaf2-2e13-4d58-e9ed-7a5ab9c9f90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faiss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2403850395.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip install faiss-gpu\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import inception_v3\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download and unzip dataset\n",
        "!wget -O data.zip \"https://drive.google.com/uc?export=download&id=1q3dpti5aX4LdD3Mq7bZ4rjTeZbQEljiy\"\n",
        "!unzip \"data.zip\"\n",
        "\n",
        "# Folder containing images\n",
        "image_folder = '/content/image_dataset'\n",
        "\n",
        "# Get a list of all image files in the folder\n",
        "image_files = glob(os.path.join(image_folder, '*.jpg'))\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
        "print(f\"Train images: {len(train_files)}, Test images: {len(test_files)}\")\n",
        "\n",
        "\n",
        "# Function to generate embeddings using Inception model\n",
        "def generate_inception_embeddings(image_paths, model, preprocess, device):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for img_path in image_paths:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image = preprocess(image).unsqueeze(0).to(device)\n",
        "            embedding = model(image).cpu().numpy().flatten()\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "    return np.array(embeddings, dtype=np.float32)\n",
        "\n",
        "\n",
        "# Load Inception model and preprocessing pipeline\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = inception_v3(pretrained=True, transform_input=True).to(device)\n",
        "model.fc = torch.nn.Identity()  # Remove classification head to get embeddings\n",
        "\n",
        "# Define image preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_inception_embeddings(train_files, model, preprocess, device)\n",
        "test_embeddings = generate_inception_embeddings(test_files, model, preprocess, device)\n",
        "\n",
        "\n",
        "# Function to create FAISS index\n",
        "def create_faiss_index(embeddings, image_paths):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    index = faiss.IndexIDMap(index)\n",
        "\n",
        "    # Add vectors to the index with IDs\n",
        "    index.add_with_ids(embeddings, np.array(range(len(image_paths))))\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "# Create FAISS index for the train set\n",
        "train_index = create_faiss_index(train_embeddings, train_files)\n",
        "\n",
        "\n",
        "# Function to retrieve similar images\n",
        "def retrieve_similar_images(query_path, preprocess, model, index, image_paths, top_k=3):\n",
        "    query_image = Image.open(query_path).convert('RGB')\n",
        "    query_tensor = preprocess(query_image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        query_features = model(query_tensor).cpu().numpy().astype(np.float32).reshape(1, -1)\n",
        "\n",
        "    distances, indices = index.search(query_features, top_k)\n",
        "    retrieved_images = [image_paths[int(idx)] for idx in indices[0]]\n",
        "\n",
        "    return retrieved_images\n",
        "\n",
        "\n",
        "# Function to calculate top-k accuracy\n",
        "def calculate_accuracy(test_files, train_index, train_files, model, preprocess, top_k=3):\n",
        "    correct_matches = 0\n",
        "\n",
        "    for test_path in test_files:\n",
        "        # Ground truth: file belongs to the same category (e.g., same folder)\n",
        "        true_category = os.path.basename(test_path).split('-')[0]  # Example: \"pexels-cat.jpg\" -> \"pexels\"\n",
        "\n",
        "        # Retrieve similar images\n",
        "        retrieved_images = retrieve_similar_images(test_path, preprocess, model, train_index, train_files, top_k)\n",
        "\n",
        "        # Check if any retrieved image belongs to the same category\n",
        "        retrieved_categories = [os.path.basename(img).split('-')[0] for img in retrieved_images]\n",
        "        if true_category in retrieved_categories:\n",
        "            correct_matches += 1\n",
        "\n",
        "    accuracy = correct_matches / len(test_files)\n",
        "    print(f\"Top-{top_k} accuracy: {accuracy:.2f}\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "calculate_accuracy(test_files, train_index, train_files, model, preprocess, top_k=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test files: {len(test_files)}\")\n",
        "print(f\"Train files: {len(train_files)}\")"
      ],
      "metadata": {
        "id": "CkmQUMNqDABO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder containing images\n",
        "image_folder = '/content/image_dataset'\n",
        "\n",
        "# Get a list of all image files in the folder\n",
        "image_files = glob(os.path.join(image_folder, '*.jpg'))\n",
        "\n",
        "# Ensure image_files is not empty\n",
        "assert len(image_files) > 0, \"No images found in the dataset folder!\"\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
        "\n",
        "# Debugging: Print lengths\n",
        "print(f\"Train images: {len(train_files)}, Test images: {len(test_files)}\")\n"
      ],
      "metadata": {
        "id": "z3MZ5idfDaqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate recall per query\n",
        "def calculate_recall(test_files, train_index, train_files, model, preprocess, top_k=3):\n",
        "    recalls = []  # To store recall for each query\n",
        "\n",
        "    for test_path in test_files:\n",
        "        # Ground truth: all images in the same category (e.g., same folder or prefix)\n",
        "        true_category = os.path.basename(test_path).split('-')[0]  # Example: \"pexels-cat.jpg\" -> \"pexels\"\n",
        "        relevant_images = [img for img in train_files if os.path.basename(img).split('-')[0] == true_category]\n",
        "        total_relevant = len(relevant_images)\n",
        "\n",
        "        # Retrieve top-k similar images\n",
        "        retrieved_images = retrieve_similar_images(test_path, preprocess, model, train_index, train_files, top_k)\n",
        "\n",
        "        # Count relevant images in the top-k retrieved results\n",
        "        retrieved_relevant_count = sum(1 for img in retrieved_images if os.path.basename(img).split('-')[0] == true_category)\n",
        "\n",
        "        # Calculate recall for this query\n",
        "        recall = retrieved_relevant_count / total_relevant if total_relevant > 0 else 0\n",
        "        recalls.append(recall)\n",
        "\n",
        "        print(f\"Query: {os.path.basename(test_path)} | Recall: {recall:.2f}\")\n",
        "\n",
        "    # Average recall across all queries\n",
        "    mean_recall = sum(recalls) / len(recalls)\n",
        "    print(f\"Mean Recall across all queries: {mean_recall:.2f}\")\n",
        "    return recalls, mean_recall\n",
        "\n",
        "\n",
        "# Example Usage: Calculate recall\n",
        "recalls, mean_recall = calculate_recall(test_files, train_index, train_files, model, preprocess, top_k=3)\n"
      ],
      "metadata": {
        "id": "zGD27yPfCigy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Folder containing images\n",
        "image_folder = '/content/image_dataset'\n",
        "\n",
        "# Get a list of all image files in the folder\n",
        "image_files = glob(os.path.join(image_folder, '*.jpg'))\n",
        "\n",
        "# Ensure image_files is not empty\n",
        "assert len(image_files) > 0, \"No images found in the dataset folder!\"\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
        "\n",
        "# Debugging: Print dataset split info\n",
        "print(f\"Train images: {len(train_files)}, Test images: {len(test_files)}\")\n",
        "print(f\"Example train file: {train_files[0]}\")\n",
        "print(f\"Example test file: {test_files[0]}\")\n",
        "\n",
        "# Rest of the recall and accuracy calculation code goes here...\n"
      ],
      "metadata": {
        "id": "D_Ap2v3MEFvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I49DGlLpH-VX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}